{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMOnea5h9AqlR3ysuDZD1qV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/leilafarsani/Text-Mining---NLP/blob/starter/Copy_of_TextMining.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##NLTK\n",
        "\n",
        "Natural Language Toolkit (NLTK)\n",
        "\n",
        "[https://www.nltk.org/](https://www.nltk.org/)\n",
        "\n",
        "*   NLTK is a platform for building Python programs to work with human language data.\n",
        "\n"
      ],
      "metadata": {
        "id": "-q5r2PY8WR_5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n"
      ],
      "metadata": {
        "id": "U-8G_TbHWhtr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print (nltk.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XWaZUpRWWtYE",
        "outputId": "66de5d53-607f-409d-a789-8114e865febc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.9.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download necessary resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rG3GFiNbamIK",
        "outputId": "c20f678a-2eaf-4677-e497-c668d66f062b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample text\n",
        "text1 = 'Hi, I am Leila. I am starting to learn Text Mining.'"
      ],
      "metadata": {
        "id": "0LOrRysuWuoq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sent_tokenize(text1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DLnVbd8FZ2BK",
        "outputId": "6ccc8da5-80d1-4975-a409-cdaf93847e62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hi, I am Leila.', 'I am starting to learn Text Mining.']"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_tokenize(text1)"
      ],
      "metadata": {
        "id": "fkcOpKa34FlS",
        "outputId": "c74b234f-8ee8-4f40-b2c7-8efd8bc483f4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hi',\n",
              " ',',\n",
              " 'I',\n",
              " 'am',\n",
              " 'Leila',\n",
              " '.',\n",
              " 'I',\n",
              " 'am',\n",
              " 'starting',\n",
              " 'to',\n",
              " 'learn',\n",
              " 'Text',\n",
              " 'Mining',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Advanced Text Preprocessing with NLTK\n",
        "In this section, we'll expand our text preprocessing capabilities beyond basic sentence tokenization."
      ],
      "metadata": {
        "id": "qcQkBthO5fjv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Word tokenization - breaking text into individual words\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "text1 = 'Hi, I am Leila. I am starting to learn Text Mining.'\n",
        "\n",
        "# Tokenize into words\n",
        "words = word_tokenize(text1)\n",
        "print(\"Words:\", words)"
      ],
      "metadata": {
        "id": "K7qH3dLY5grZ",
        "outputId": "552f5b1a-665e-459c-93bb-c4f64c096037",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Words: ['Hi', ',', 'I', 'am', 'Leila', '.', 'I', 'am', 'starting', 'to', 'learn', 'Text', 'Mining', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Add stop word removal:"
      ],
      "metadata": {
        "id": "tXybh5yG6DB_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the stopwords resource\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Get English stop words\n",
        "stop_words = set(stopwords.words('english'))\n",
        "print(\"Some common stop words:\", list(stop_words)[:10])\n",
        "\n",
        "# Remove stop words from our text\n",
        "filtered_words = [word for word in words if word.lower() not in stop_words]\n",
        "print(\"Original words:\", words)\n",
        "print(\"After removing stop words:\", filtered_words)"
      ],
      "metadata": {
        "id": "dagTaIHM5863",
        "outputId": "3c758d06-e618-4cdc-fb38-b06ded2f599e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Some common stop words: ['couldn', \"he'd\", 'once', 'with', 'before', 'above', 'not', 'how', \"won't\", 'over']\n",
            "Original words: ['Hi', ',', 'I', 'am', 'Leila', '.', 'I', 'am', 'starting', 'to', 'learn', 'Text', 'Mining', '.']\n",
            "After removing stop words: ['Hi', ',', 'Leila', '.', 'starting', 'learn', 'Text', 'Mining', '.']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample text\n",
        "text2 = \"Natural language processing is fascinating. It helps computers understand human language.\""
      ],
      "metadata": {
        "id": "EI_RDSxjd2pP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize into sentences\n",
        "sentences = sent_tokenize(text1)\n",
        "print(\"Sentences:\", sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gab7H4QHeIr_",
        "outputId": "4bb1529b-d0c7-4fff-f728-08411414c0bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentences: ['Hi, I am Leila.', 'I am starting to learn Text Mining.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize into words\n",
        "words = word_tokenize(text2)\n",
        "print(\"Words:\", words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oMqlyLYreeu0",
        "outputId": "89f09bb9-dedc-4c5c-f0ee-524e1f7ff2de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Words: ['Natural', 'language', 'processing', 'is', 'fascinating', '.', 'It', 'helps', 'computers', 'understand', 'human', 'language', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove stop words\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_words = [word for word in words if word.lower() not in stop_words]\n",
        "print(\"After removing stop words:\", filtered_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VcGG-9LOekMB",
        "outputId": "30116cd4-3482-4cc7-894a-08e90eef734a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After removing stop words: ['Natural', 'language', 'processing', 'fascinating', '.', 'helps', 'computers', 'understand', 'human', 'language', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Stemming\n",
        "stemmer = PorterStemmer()\n",
        "stemmed_words = [stemmer.stem(word) for word in filtered_words]\n",
        "print(\"After stemming:\", stemmed_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MEWsCfloenyg",
        "outputId": "f4ccfada-6472-495a-a6e9-256ccb2af616"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After stemming: ['natur', 'languag', 'process', 'fascin', '.', 'help', 'comput', 'understand', 'human', 'languag', '.']\n"
          ]
        }
      ]
    }
  ]
}